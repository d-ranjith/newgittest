#!/bin/bash

namespace="$1"
timestamp=$(date +"%Y%m%d%H%M%S")
output_file="pod_startup_times_${timestamp}.csv"

if [[ -z "$namespace" ]]; then
  echo "Please provide a namespace as an argument."
  exit 1
fi

# Get the pod list
pods=$(kubectl get pods -n "$namespace" --no-headers -o custom-columns=":metadata.name")

# Create the CSV file and write the header
echo "Pod Name,Startup Time (seconds),Status" > "$output_file"

# Iterate over each pod
for pod in $pods; do
    echo "Processing pod: $pod"

    # Get the pod's creation time
    creation_time=$(kubectl get pod "$pod" -n "$namespace" -o jsonpath='{.metadata.creationTimestamp}')

    # Get the pod's start time from events
    start_time=""
    while IFS= read -r line; do
        event_type=$(echo "$line" | awk '{print $3}')
        if [[ "$event_type" == "Started" ]]; then
            start_time=$(echo "$line" | awk '{print $1}')
            break
        fi
    done < <(kubectl get events -n "$namespace" --field-selector involvedObject.name="$pod" --sort-by='.lastTimestamp' --no-headers -o custom-columns=":firstTimestamp" | tac)

    if [[ -z "$start_time" ]]; then
        echo "No startup event found for pod: $pod"
        continue
    fi

    # Calculate the startup time
    creation_timestamp=$(date -d "$creation_time" +%s)
    start_timestamp=$(date -d "$start_time" +%s)
    startup_time=$((start_timestamp - creation_timestamp))

    # Get the pod's status
    pod_status=$(kubectl get pod "$pod" -n "$namespace" -o jsonpath='{.status.phase}')

    echo "Pod: $pod, Startup time: $startup_time seconds, Status: $pod_status"

    # Append the pod name, startup time, and status to the CSV file
    echo "$pod,$startup_time,$pod_status" >> "$output_file"
done

echo "Output written to $output_file"









$folder = "C:\path\to\folder"

Get-ChildItem -Path $folder | ForEach-Object {
    $oldName = $_.Name
    $newName = $_.LastWriteTime.ToString("yyyyMMdd") + "_" + $_.BaseName.Replace("-","_") + $_.Extension
    Rename-Item -Path $_.FullName -NewName $newName -WhatIf
    Write-Host "Renamed $oldName to $newName"
}



const schema = Joi.object({
  AenvClass: Joi.string().optional(),
  BenvClass: Joi.string().optional(),
  CenvClass: Joi.string().optional(),
  serverclass: Joi.string().when(
    Joi.object({
      AenvClass: Joi.string().optional(),
      BenvClass: Joi.string().optional(),
      CenvClass: Joi.string().optional()
    }).or('AenvClass', 'BenvClass', 'CenvClass'),
    {
      then: Joi.forbidden(),
      otherwise: Joi.required()
    }
  )
});

const validInput = {
  AenvClass: 'envA',
  serverclass: 'class1'
};

const invalidInput = {
  serverclass: 'class2'
};

describe('Schema Validation', () => {
  test('valid input passes schema validation', () => {
    const { error } = schema.validate(validInput);
    expect(error).toBeUndefined();
  });

  test('invalid input fails schema validation', () => {
    const { error } = schema.validate(invalidInput);
    expect(error).toBeDefined();
  });
});

oc get pvc --all-namespaces -o json | jq '.items[] | select(.status.phase == "Bound" and .spec.volumeName == null) | {Namespace: .metadata.namespace, Name: .metadata.name, Age: .metadata.creationTimestamp}'

oc get pr | grep -E 'Succeeded|Failed' | awk '{print $1}' | while read -r each_pr; do
  oc get pipelinerun "$each_pr" -ojsonpath='{.status.taskRuns[*].name}' | tr ' ' '\n' | while read -r each_tr; do
    oc describe tr "$each_tr" | awk -F':' '/pvc/{print $2}' >> pvc-list.txt
  done
done
cat pvc-list.txt | sort -u

oc get pr | grep -E 'Succeeded|Failed' | awk '{print $1}' | while read -r each_pr; do
  oc get pipelinerun "$each_pr" -ojsonpath='{.status.taskRuns[*].name}' | tr ' ' '\n' | while read -r each_tr; do
    oc describe tr "$each_tr" | awk -F':' '/pvc/{print $2}'
  done
done | sort -u



#!/bin/bash

set -e # Exit immediately if a command exits with a non-zero status

# Define the namespace to search in
NAMESPACE="<namespace>"

# List all the PipelineRuns in the namespace
PIPELINERUN_NAMES=$(oc get pipelinerun -n $NAMESPACE -o jsonpath='{.items[*].metadata.name}')

# Loop through each PipelineRun and get the PVC names used in its TaskRuns
for PIPELINERUN_NAME in $PIPELINERUN_NAMES; do
  PIPELINERUN_JSON=$(oc get pipelinerun $PIPELINERUN_NAME -n $NAMESPACE -o json)
  PIPELINERUN_STATUS=$(echo "$PIPELINERUN_JSON" | jq -r '.status.conditions[] | select(.type=="Succeeded").status')
  
  if [[ "$PIPELINERUN_STATUS" == "True" ]]; then
    PVC_NAMES=$(echo "$PIPELINERUN_JSON" | jq -r '.status.volumes[].persistentVolumeClaim.claimName')
    if [[ -n "$PVC_NAMES" ]]; then
      # Loop through the PVC names and delete each PVC
      for PVC_NAME in $PVC_NAMES; do
        oc delete pvc $PVC_NAME -n $NAMESPACE --ignore-not-found=true
        echo "Deleted PVC $PVC_NAME in PipelineRun $PIPELINERUN_NAME"
      done
    else
      TASKRUNS=$(echo "$PIPELINERUN_JSON" | jq -r '.status.taskRuns[].name')
      for TASKRUN in $TASKRUNS; do
        TASKRUN_JSON=$(oc get taskrun $TASKRUN -n $NAMESPACE -o json)
        PVC_NAMES=$(echo "$TASKRUN_JSON" | jq -r '.spec.inputs.artifacts[].from.name | select(startswith("pvc-"))')
        if [[ -n "$PVC_NAMES" ]]; then
          # Loop through the PVC names and delete each PVC
          for PVC_NAME in $PVC_NAMES; do
            oc delete pvc $PVC_NAME -n $NAMESPACE --ignore-not-found=true
            echo "Deleted PVC $PVC_NAME in TaskRun $TASKRUN"
          done
        fi
      done
    fi
  else
    echo "PipelineRun $PIPELINERUN_NAME is not completed yet."
  fi
done



#!/bin/bash

set -e # Exit immediately if a command exits with a non-zero status

# Define the PipelineRun name
PIPELINERUN_NAME="<pipelinerun-name>"

# Get the PipelineRun status
PIPELINERUN_JSON=$(oc get pipelinerun $PIPELINERUN_NAME -o json)

# Check if the PipelineRun has completed
PIPELINERUN_STATUS=$(echo "$PIPELINERUN_JSON" | jq -r '.status.conditions[] | select(.type=="Succeeded").status')

if [[ "$PIPELINERUN_STATUS" == "True" ]]; then
  # Get the PVC names used in the PipelineRun
  PVC_NAMES=$(echo "$PIPELINERUN_JSON" | jq -r '.status.volumes[].persistentVolumeClaim.claimName')
  if [[ -n "$PVC_NAMES" ]]; then
    # Loop through the PVC names and delete each PVC
    for PVC_NAME in $PVC_NAMES; do
      oc delete pvc $PVC_NAME --ignore-not-found=true
      echo "Deleted PVC $PVC_NAME"
    done
  else
    # Get the TaskRuns in the PipelineRun
    TASKRUNS=$(echo "$PIPELINERUN_JSON" | jq -r '.status.taskRuns[].name')

    # Loop through the TaskRuns and get the PVC names used in each TaskRun
    for TASKRUN in $TASKRUNS; do
      TASKRUN_JSON=$(oc get taskrun $TASKRUN -o json)
      PVC_NAMES=$(echo "$TASKRUN_JSON" | jq -r '.spec.inputs.artifacts[].from.name | select(startswith("pvc-"))')
      if [[ -n "$PVC_NAMES" ]]; then
        # Loop through the PVC names and delete each PVC
        for PVC_NAME in $PVC_NAMES; do
          oc delete pvc $PVC_NAME --ignore-not-found=true
          echo "Deleted PVC $PVC_NAME"
        done
      fi
    done
  fi
else
  echo "PipelineRun $PIPELINERUN_NAME is not completed yet."
  exit 1
fi




#!/bin/bash

# Set the Tekton namespace
TKN_NAMESPACE=<your-tekton-namespace>

# Get a list of all PVCs in the Tekton namespace
PVC_LIST=$(oc get pvc -n $TKN_NAMESPACE -o json | jq -r '.items[].metadata.name')

# Loop over each PVC and check if it's in use by any running PipelineRuns
for PVC in $PVC_LIST; do
  # Get a list of all PipelineRuns that are still running and using the current PVC
  RUNNING_RUNS=$(tkn pipelinerun list -n $TKN_NAMESPACE --condition status.conditions.status=True,status.conditions.type=Succeeded --field-selector 'status.pvcStatuses.name=='$PVC'' | awk '{print $1}')

  # If there are no running PipelineRuns that are using the current PVC, delete it
  if [[ -z "$RUNNING_RUNS" ]]; then
    echo "Deleting PVC: $PVC"
    oc delete pvc $PVC -n $TKN_NAMESPACE
  else
    echo "PVC is in use: $PVC"
  fi
done


$ oc get pvc --all-namespaces -o jsonpath='{range .items[*]}{"PVC: "}{.metadata.namespace}/{.metadata.name}{"\tBound To: "}{.spec.volumeName}{"\tUsed By: "}{range .status.accessModes[*]}{.};{end}{"\n"}{end}'
$ oc logs -f $(oc get pods -l tekton.dev/pipelineRun=<PIPELINERUN_NAME> -o name) | grep -A 1 "Mounting pvc"

#!/bin/bash

# Set the OpenShift namespace to inspect
NAMESPACE=<YOUR_NAMESPACE>

# Get a list of all running PipelineRuns in the namespace
RUNS=$(oc get pipelineruns -n $NAMESPACE -o json | jq -r '.items[] | select(.status.conditions[]?.status=="True" and .status.conditions[]?.type=="Succeeded") | .metadata.name')

# Loop through the PipelineRuns and list the PVC claims assigned to each one
for RUN in $RUNS; do
  echo "PipelineRun: $RUN"
  # Get the list of PVC claims associated with the PipelineRun
  CLAIMS=$(oc get pods -n $NAMESPACE -l tekton.dev/pipelineRun=$RUN -o json | jq -r '.items[].spec.volumes[]?.persistentVolumeClaim.claimName' | grep -v 'null')
  # Loop through the PVC claims and list their status
  for CLAIM in $CLAIMS; do
    STATUS=$(oc get pvc -n $NAMESPACE $CLAIM -o jsonpath='{.status.phase}')
    echo "  PVC: $CLAIM  Status: $STATUS"
  done
done

#!/bin/bash

# Set the OpenShift namespace to inspect
NAMESPACE=<YOUR_NAMESPACE>

# Get a list of all running PipelineRuns in the namespace
RUNS=$(oc get pipelineruns -n $NAMESPACE --field-selector=status.phase=Running -o name)

# Loop through the PipelineRuns and list the PVC claims assigned to each one
for RUN in $RUNS; do
  echo "PipelineRun: $(basename $RUN)"
  # Get the list of PVC claims associated with the PipelineRun
  CLAIMS=$(oc get pods -n $NAMESPACE -l tekton.dev/pipelineRun=$(basename $RUN) -o jsonpath='{range .items[*].spec.volumes[*]}{.persistentVolumeClaim.claimName}{"\n"}{end}' | grep -v 'null')
  # Loop through the PVC claims and list their status
  for CLAIM in $CLAIMS; do
    STATUS=$(oc get pvc -n $NAMESPACE $CLAIM -o jsonpath='{.status.phase}')
    echo "  PVC: $CLAIM  Status: $STATUS"
  done
done



apiVersion: v1
kind: ConfigMap
metadata:
  name: apigee-config
data:
  apigee-org: my-apigee-org
  apigee-env: my-apigee-env
  apigee-username: my-apigee-username
  apigee-password: my-apigee-password
  apigee-proxy-name: my-apigee-proxy-name
  apigee-proxy-url: https://my-apigee-proxy-url.com
---
apiVersion: v1
kind: Secret
metadata:
  name: apigee-secret
type: Opaque
data:
  apigee-password: my-apigee-password-encrypted
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: node-app
  template:
    metadata:
      labels:
        app: node-app
    spec:
      containers:
        - name: node-app
          image: my-node-app-image
          env:
            - name: APIGEE_ORG
              valueFrom:
                configMapKeyRef:
                  name: apigee-config
                  key: apigee-org
            - name: APIGEE_ENV
              valueFrom:
                configMapKeyRef:
                  name: apigee-config
                  key: apigee-env
            - name: APIGEE_USERNAME
              valueFrom:
                configMapKeyRef:
                  name: apigee-config
                  key: apigee-username
            - name: APIGEE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: apigee-secret
                  key: apigee-password
            - name: APIGEE_PROXY_NAME
              valueFrom:
                configMapKeyRef:
                  name: apigee-config
                  key: apigee-proxy-name
            - name: APIGEE_PROXY_URL
              valueFrom:
                configMapKeyRef:
                  name: apigee-config
                  key: apigee-proxy-url



helm install my-apigee-edge-job ./ --values values.yaml


apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "apigee-edge.fullname" . }}-config
  labels:
    {{- include "apigee-edge.labels" . | nindent 4 }}
data:
  org: {{ .Values.apigee.org }}
  env: {{ .Values.apigee.env }}
  user: {{ .Values.apigee.user }}
  pass: {{ .Values.apigee.pass }}
  baseurl: {{ .Values.apigee.baseurl }}
  proxy_name: {{ .Values.apigee.proxy_name }}
  proxy_bundle_path: {{ .Values.apigee.proxy_bundle_path }}



apiVersion: v1
kind: Secret
metadata:
  name: {{ include "apigee-edge.fullname" . }}-secret
  labels:
    {{- include "apigee-edge.labels" . | nindent 4 }}
type: Opaque
data:
  user: {{ .Values.apigee.user | b64enc }}
  pass: {{ .Values.apigee.pass | b64enc }}


apiVersion: v1
kind: ConfigMap
metadata:
  name: apigee-configmap
data:
  sector: "{{ .Values.apigeeSector }}"
  env: "{{ .Values.apigeeEnv }}"
  fid: "{{ .Values.apigeeFid }}"
  region: "{{ .Values.apigeeRegion }}"
  apiBaseVersion: "{{ .Values.apigeeApiBaseVersion }}"
  proxyName: "{{ .Values.apigeeProxyName }}"
  baseUrl: "{{ .Values.apigeeBaseUrl }}"
  proxyBundlePath: "{{ .Values.apigeeProxyBundlePath }}"
  apiPollIntervals: "{{ .Values.apigeeApiPollIntervals }}"
  maxPollAttemptValue: "{{ .Values.apigeeMaxPollAttemptValue }}"


apiVersion: v1
kind: Secret
metadata:
  name: {{ .Values.artifactorySecretName }}
type: Opaque
stringData:
  username: "your-artifactory-username"
  password: "your-artifactory-password"
---
apiVersion: v1
kind: Secret
metadata:
  name: {{ .Values.apigeeSecretName }}
type: Opaque
stringData:
  secret: "your-apigee-secret"
  apikey: "your-apigee-apikey"
  certs: "your-apigee-certs"
  keys: "your-apigee-keys"


jobName: "apigee-edge-job"
timestamp: {{ .Release.Time.Seconds }}
image:
  repository: "your-docker-repo/your-image"
  tag: "latest"
  pullPolicy: "IfNotPresent"
artifactorySecretName: "artifactory-secret"
apigeeSector: "your-apigee-sector"
apigeeEnv: "your-apigee-environment"
apigeeFid: "your-apigee-fid"
apigeeRegion: "your-apigee-region"
apigeeSecretName: "apigee-secret"
apigeeApiBaseVersion: "your-apigee-api-base-version"
apigeeProxyName: "your-apigee-proxy-name"
apigeeBaseUrl: "your-apigee-base-url"
apigeeProxyBundlePath: "your-apigee-proxy-bundle-path"
apigeeCerts: "your-apigee-certs"
apigeeKeys: "your-apigee-keys"
apigeeApiPollIntervals: "your-apigee-api-poll-intervals"
apigeeMaxPollAttemptValue: "your-apigee-max-poll-attempt-value"


apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Values.jobName }}-{{ .Values.timestamp }}
spec:
  template:
    spec:
      containers:
        - name: {{ .Values.jobName }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          env:
            - name: ARTIFACTORY_USERNAME
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.artifactorySecretName }}
                  key: username
            - name: ARTIFACTORY_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.artifactorySecretName }}
                  key: password
            - name: APIGEE_SECTOR
              valueFrom:
                configMapKeyRef:
                  name: apigee-configmap
                  key: sector
            - name: APIGEE_ENV
              valueFrom:
                configMapKeyRef:
                  name: apigee-configmap
                  key: env
            - name: APIGEE_FID
              valueFrom:
                configMapKeyRef:
                  name: apigee-configmap
                  key: fid
            - name: APIGEE_REGION
              valueFrom:
                configMapKeyRef:
                  name: apigee-configmap
                  key: region
            - name: APIGEE_SECRET
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.apigeeSecretName }}
                  key: secret
            - name: APIGEE_APIKEY
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.apigeeSecretName }}
                  key: apikey
            - name: APIGEE_API_BASE_VERSION
              valueFrom:
                configMapKeyRef:
                  name: apigee-configmap
                  key: apiBaseVersion
            - name: APIGEE_PROXY_NAME
              valueFrom:
                configMapKeyRef:
                  name: apigee-configmap
                  key: proxyName
            - name: APIGEE_BASE_URL
              valueFrom:
                configMapKeyRef:
                  name: apigee-configmap
                  key: baseUrl
            - name: APIGEE_PROXY_BUNDLE_PATH
              valueFrom:
                configMapKeyRef:
                  name: apigee-configmap
                  key: proxyBundlePath
            - name: APIGEE_CERTS
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.apigeeSecretName }}
                  key: certs
            - name: APIGEE_KEYS
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.apigeeSecretName }}
                  key: keys
            - name: APIGEE_API_POLL_INTERVALS
              valueFrom:
                configMapKeyRef:
                  name: apigee-configmap
                  key: apiPollIntervals
            - name: APIGEE_MAX_POLL_ATTEMPT_VALUE
              valueFrom:
                configMapKeyRef:
                  name: apigee-configmap
                  key: maxPollAttemptValue
          command: [ "node" ]
          args: [ "index.js" ]
      restartPolicy: Never
  backoffLimit: 0



===============================
const axios = require('axios');
const fs = require('fs');
const path = require('path');
const AdmZip = require('adm-zip');
const retry = require('async-retry');

const ARTIFACTORY_URL = process.env.ARTIFACTORY_URL;
const ARTIFACTORY_USERNAME = process.env.ARTIFACTORY_USERNAME;
const ARTIFACTORY_PASSWORD = process.env.ARTIFACTORY_PASSWORD;
const FILE_PATH = process.env.FILE_PATH; // the path to save the downloaded file

const downloadArtifactoryData = async () => {
  const response = await retry(async () => {
    const { status, data } = await axios.get(ARTIFACTORY_URL, {
      auth: {
        username: ARTIFACTORY_USERNAME,
        password: ARTIFACTORY_PASSWORD,
      },
      responseType: 'arraybuffer', // Set the response type to arraybuffer to download the file
    });
    if (status >= 200 && status < 300) {
      return data;
    }
    throw new Error(`Failed to fetch data from Artifactory with status code: ${status}`);
  }, {
    retries: 3, // Number of retry attempts
    factor: 2, // Factor by which the delay increases on each retry attempt
    minTimeout: 1000, // Minimum delay between retry attempts (in milliseconds)
    maxTimeout: 60000, // Maximum delay between retry attempts (in milliseconds)
    onRetry: (err) => console.log(`Retrying download: ${err}`), // Function to call on each retry attempt
  });
  
  // Create the directory if it doesn't exist
  if (!fs.existsSync(path.dirname(FILE_PATH))) {
    fs.mkdirSync(path.dirname(FILE_PATH), { recursive: true });
  }
  
  // Write the downloaded file to disk
  const zip = new AdmZip(response);
  zip.extractAllTo(path.dirname(FILE_PATH), true);
  
  console.log(`File downloaded and extracted to ${FILE_PATH}`);
};

downloadArtifactoryData().catch((err) => {
  console.error(`Error downloading from Artifactory: ${err}`);
});


-------------------
const fs = require('fs');
const axios = require('axios');
const { createProxyPayload } = require('./createProxyPayload');
const { log } = require('./appLogger');

const APIGEE_URL = process.env.APIGEE_URL;
const ORG_NAME = process.env.ORG_NAME;
const ENV_NAME = process.env.ENV_NAME;
const PROXY_NAME = process.env.PROXY_NAME;
const APIGEE_USERNAME = process.env.APIGEE_USERNAME;
const APIGEE_PASSWORD = process.env.APIGEE_PASSWORD;
const KEY_FILE = process.env.KEY_FILE;
const CERT_FILE = process.env.CERT_FILE;
const TOKEN = process.env.TOKEN;
const BUNDLE_FILE = process.env.BUNDLE_FILE;

// Create the Apigee proxy
async function createProxy() {
  const proxyPayload = createProxyPayload(PROXY_NAME);
  const authHeader = `Basic ${Buffer.from(`${APIGEE_USERNAME}:${APIGEE_PASSWORD}`).toString('base64')}`;

  try {
    const response = await axios.post(`${APIGEE_URL}/v1/organizations/${ORG_NAME}/environments/${ENV_NAME}/apis`, proxyPayload, {
      headers: {
        Authorization: authHeader,
        'Content-Type': 'application/json',
      },
    });
    if (response.status === 201) {
      const revision = response.data.revision;
      log.info(`Created Apigee proxy with revision ${revision}`);
      return revision;
    } else {
      const error = `Failed to create Apigee proxy: HTTP ${response.status}`;
      log.error(error);
      throw new Error(error);
    }
  } catch (error) {
    log.error(`Failed to create Apigee proxy: ${error}`);
    throw error;
  }
}

// Upload the Apigee proxy bundle
async function uploadBundle(revision) {
  const authHeader = `Bearer ${TOKEN}`;
  const formData = {
    'file': fs.createReadStream(BUNDLE_FILE),
  };

  try {
    const response = await axios.put(`${APIGEE_URL}/v1/organizations/${ORG_NAME}/apis/${PROXY_NAME}/revisions/${revision}/deployments?override=true`, formData, {
      headers: {
        Authorization: authHeader,
        'Content-Type': 'multipart/form-data',
      },
      maxContentLength: Infinity,
      maxBodyLength: Infinity,
    });
    if (response.status === 200) {
      log.info(`Uploaded Apigee proxy bundle with revision ${revision}`);
    } else {
      const error = `Failed to upload Apigee proxy bundle: HTTP ${response.status}`;
      log.error(error);
      throw new Error(error);
    }
  } catch (error) {
    log.error(`Failed to upload Apigee proxy bundle: ${error}`);
    throw error;
  }
}

// Main function to create the proxy and upload the bundle
async function main() {
  try {
    const revision = await createProxy();
    await uploadBundle(revision);
  } catch (error) {
    log.error(`Failed to deploy Apigee proxy: ${error}`);
    process.exit(1);
  }
}

// Run the main function
main();

============
const fs = require('fs');
const axios = require('axios');
const { createProxyPayload } = require('./createProxyPayload');
const { log } = require('./appLogger');

const APIGEE_URL = process.env.APIGEE_URL;
const ORG_NAME = process.env.ORG_NAME;
const ENV_NAME = process.env.ENV_NAME;
const PROXY_NAME = process.env.PROXY_NAME;
const APIGEE_USERNAME = process.env.APIGEE_USERNAME;
const APIGEE_PASSWORD = process.env.APIGEE_PASSWORD;
const KEY_FILE = process.env.KEY_FILE;
const CERT_FILE = process.env.CERT_FILE;
const TOKEN = process.env.TOKEN;
const BUNDLE_FILE = process.env.BUNDLE_FILE;

// Create the Apigee proxy
async function createProxy() {
  const proxyPayload = createProxyPayload(PROXY_NAME);
  const authHeader = `Basic ${Buffer.from(`${APIGEE_USERNAME}:${APIGEE_PASSWORD}`).toString('base64')}`;

  try {
    const response = await axios.post(`${APIGEE_URL}/v1/organizations/${ORG_NAME}/environments/${ENV_NAME}/apis`, proxyPayload, {
      headers: {
        Authorization: authHeader,
        'Content-Type': 'application/json',
      },
    });
    return response.data.revision;
  } catch (error) {
    log.error(`Failed to create Apigee proxy: ${error}`);
    throw error;
  }
}

// Upload the Apigee proxy bundle
async function uploadBundle(revision) {
  const authHeader = `Bearer ${TOKEN}`;
  const formData = {
    'file': fs.createReadStream(BUNDLE_FILE),
  };

  try {
    const response = await axios.put(`${APIGEE_URL}/v1/organizations/${ORG_NAME}/apis/${PROXY_NAME}/revisions/${revision}/deployments?override=true`, formData, {
      headers: {
        Authorization: authHeader,
        'Content-Type': 'multipart/form-data',
      },
      maxContentLength: Infinity,
      maxBodyLength: Infinity,
    });
    log.info(`Uploaded Apigee proxy bundle with revision ${revision}: ${response.data}`);
  } catch (error) {
    log.error(`Failed to upload Apigee proxy bundle: ${error}`);
    throw error;
  }
}

// Main function to create the proxy and upload the bundle
async function main() {
  try {
    const revision = await createProxy();
    log.info(`Created Apigee proxy with revision ${revision}`);
    await uploadBundle(revision);
  } catch (error) {
    log.error(`Failed to deploy Apigee proxy: ${error}`);
    process.exit(1);
  }
}

// Run the main function
main();


-------------------------------------

const axios = require('axios');
const fs = require('fs');
const FormData = require('form-data');
const pino = require('pino')({ level: process.env.LOG_LEVEL });

// Helper function to make authenticated requests to Apigee
async function makeRequest(url, method, data, headers) {
  const auth = {
    username: process.env.APIGEE_USERNAME,
    password: process.env.APIGEE_PASSWORD
  };

  const response = await axios({
    url,
    method,
    data,
    headers,
    auth
  });

  return response.data;
}

async function main() {
  // Read the proxy bundle file
  const bundle = fs.readFileSync(process.env.BUNDLE_PATH);

  // Create the proxy and get the revision ID
  const createProxyUrl = `https://api.enterprise.apigee.com/v1/organizations/${process.env.APIGEE_ORG}/apis`;
  const createProxyData = {
    name: process.env.PROXY_NAME,
    proxyBundle: bundle.toString('base64')
  };
  const createProxyHeaders = {
    'Content-Type': 'application/json'
  };

  pino.info('Creating Apigee proxy...');
  const { revision } = await makeRequest(createProxyUrl, 'POST', createProxyData, createProxyHeaders);

  // Upload the bundle using the revision ID
  const uploadUrl = `https://api.enterprise.apigee.com/v1/organizations/${process.env.APIGEE_ORG}/apis/${process.env.PROXY_NAME}/revisions/${revision}/proxies/${process.env.PROXY_NAME}/bundle`;
  const uploadHeaders = {
    'Content-Type': 'multipart/form-data'
  };
  const uploadFormData = new FormData();
  uploadFormData.append('file', bundle);

  pino.info('Uploading proxy bundle...');
  await makeRequest(uploadUrl, 'POST', uploadFormData, uploadHeaders);

  pino.info(`Proxy bundle uploaded to revision ${revision}.`);
}

main().catch((err) => {
  pino.error(err);
  process.exit(1);
});

================================
const fs = require('fs');
const axios = require('axios');
const dotenv = require('dotenv');
const pino = require('pino')({
  level: 'info',
  prettyPrint: { colorize: true },
});

// Load environment variables from .env file or Helm chart
dotenv.config();

// Set up authentication headers
const auth = {
  username: process.env.APIGEE_USERNAME,
  password: process.env.APIGEE_PASSWORD,
  client_id: process.env.APIGEE_CLIENT_ID,
  client_secret: process.env.APIGEE_CLIENT_SECRET,
  token: process.env.APIGEE_TOKEN,
  cert: fs.readFileSync(process.env.APIGEE_CERT_PATH),
  key: fs.readFileSync(process.env.APIGEE_KEY_PATH),
  passphrase: process.env.APIGEE_PASSPHRASE,
};

// Create Apigee proxy and get revision ID
async function createProxy() {
  try {
    const response = await axios.post(
      `https://api.enterprise.apigee.com/v1/organizations/${process.env.APIGEE_ORG}/apis`,
      {
        name: process.env.APIGEE_PROXY_NAME,
        connectionType: 'none',
      },
      { auth }
    );
    const revisionId = response.data.revision[0].name;
    pino.info(`Apigee proxy created with revision ID ${revisionId}`);
    return revisionId;
  } catch (error) {
    pino.error(`Error creating Apigee proxy: ${error.message}`);
    throw error;
  }
}

// Upload Apigee bundle
async function uploadBundle(revisionId) {
  try {
    const bundle = fs.createReadStream(process.env.APIGEE_BUNDLE_PATH);
    const response = await axios.put(
      `https://api.enterprise.apigee.com/v1/organizations/${process.env.APIGEE_ORG}/apis/${process.env.APIGEE_PROXY_NAME}/revisions/${revisionId}/deployments`,
      bundle,
      {
        headers: {
          'Content-Type': 'application/octet-stream',
        },
        auth,
      }
    );
    pino.info(`Apigee bundle uploaded with status code ${response.status}`);
  } catch (error) {
    pino.error(`Error uploading Apigee bundle: ${error.message}`);
    throw error;
  }
}

// Main function
async function main() {
  try {
    const revisionId = await createProxy();
    await uploadBundle(revisionId);
  } catch (error) {
    pino.error(`An error occurred: ${error.message}`);
    process.exit(1);
  }
}

main();


======================

const axios = require('axios');
const FormData = require('form-data');
const fs = require('fs');
const { createLogger, format, transports } = require('winston');
const { combine, timestamp, printf } = format;

const env = process.env;

// Configure logger
const myFormat = printf(({ level, message, timestamp }) => {
  return `${timestamp} [${level}]: ${message}`;
});
const logger = createLogger({
  level: 'info',
  format: combine(
    timestamp(),
    myFormat
  ),
  transports: [
    new transports.Console()
  ]
});

// Read configuration from environment variables or Helm chart values
const certPath = env.CERT_PATH || '/etc/ssl/certs/server.crt';
const keyPath = env.KEY_PATH || '/etc/ssl/private/server.key';
const certPwd = env.CERT_PASSWORD || 'mypassword';
const token = env.APIGEE_TOKEN || 'mytoken';
const org = env.APIGEE_ORG || 'myorg';
const envName = env.APIGEE_ENV || 'myenv';
const apiName = env.APIGEE_API || 'myapi';
const bundlePath = env.BUNDLE_PATH || './bundle.zip';
const uploadPath = env.APIGEE_UPLOAD_PATH || `/organizations/${org}/apis/${apiName}/revisions`;

// Validate configuration
if (!fs.existsSync(certPath)) {
  logger.error(`Certificate file not found at ${certPath}`);
  process.exit(1);
}
if (!fs.existsSync(keyPath)) {
  logger.error(`Key file not found at ${keyPath}`);
  process.exit(1);
}
if (!fs.existsSync(bundlePath)) {
  logger.error(`Bundle file not found at ${bundlePath}`);
  process.exit(1);
}

// Read certificate and key files
const cert = fs.readFileSync(certPath);
const key = fs.readFileSync(keyPath);

// Create form data object with bundle file
const formData = new FormData();
formData.append('file', fs.createReadStream(bundlePath));

// Configure Axios
const api = axios.create({
  baseURL: `https://api.enterprise.apigee.com/v1${uploadPath}`,
  headers: {
    'Content-Type': 'multipart/form-data',
    'Authorization': `Bearer ${token}`
  },
  httpsAgent: new https.Agent({
    cert: cert,
    key: key,
    passphrase: certPwd
  })
});

// Create proxy revision
api.post(`/organizations/${org}/apis/${apiName}/revisions`, {})
  .then(response => {
    const revision = response.data.revision;
    logger.info(`Created revision ${revision} for API ${apiName}`);
    // Upload bundle to revision
    return api.put(`/organizations/${org}/apis/${apiName}/revisions/${revision}/deployments?action=import&name=${envName}`, formData, {
      headers: {
        'Content-Type': `multipart/form-data; boundary=${formData._boundary}`
      },
      maxContentLength: Infinity,
      maxBodyLength: Infinity
    });
  })
  .then(response => {
    logger.info(`Uploaded bundle to API ${apiName} revision ${response.data.revision}`);
  })
  .catch(error => {
    logger.error(`Error uploading bundle to API ${apiName}: ${error}`);
  });

====================
const axios = require('axios');
const FormData = require('form-data');
const fs = require('fs');
const path = require('path');
const { createLogger, format, transports } = require('winston');
const { combine, timestamp, printf } = format;
const https = require('https');
const { promisify } = require('util');

// Create a log formatter that outputs logs in ISO format
const isoLogFormatter = printf(({ level, message, timestamp }) => {
  return `${timestamp} [${level.toUpperCase()}] ${message}`;
});

// Create a logger that logs to the console and a file in ISO format
const logger = createLogger({
  format: combine(
    timestamp(),
    isoLogFormatter
  ),
  transports: [
    new transports.Console(),
    new transports.File({ filename: 'logs/app.log' })
  ]
});

// Read environment variables from the .env file if it exists, and fall back to Helm chart environment variables
require('dotenv').config();
const env = process.env;
const {
  APIGEE_ORG = env.HELM_APIGEE_ORG,
  APIGEE_ENV = env.HELM_APIGEE_ENV,
  APIGEE_API_VERSION = env.HELM_APIGEE_API_VERSION || 'v1',
  APIGEE_BASE_URL = env.HELM_APIGEE_BASE_URL || 'https://api.enterprise.apigee.com',
  APIGEE_PROXY_NAME = env.HELM_APIGEE_PROXY_NAME,
  APIGEE_BUNDLE_PATH = env.HELM_APIGEE_BUNDLE_PATH || './myproxy.zip',
  APIGEE_CERT_PATH = env.HELM_APIGEE_CERT_PATH || './cert.pem',
  APIGEE_CERT_PASSWORD = env.HELM_APIGEE_CERT_PASSWORD,
  APIGEE_TOKEN = env.HELM_APIGEE_TOKEN,
  APIGEE_POLL_INTERVAL = env.HELM_APIGEE_POLL_INTERVAL || 1000, // default to 1 second polling interval
  APIGEE_MAX_POLL_ATTEMPTS = env.HELM_APIGEE_MAX_POLL_ATTEMPTS || 10 // default to 10 polling attempts
} = env;

// Validate required environment variables are set
if (!APIGEE_BASE_URL || !APIGEE_ORG || !APIGEE_PROXY_NAME || !APIGEE_ENV || !APIGEE_TOKEN || !APIGEE_CERT_PATH || !APIGEE_CERT_PASSWORD || !APIGEE_BUNDLE_PATH) {
  logger.error('Missing required environment variable(s). Check .env file or Helm chart configuration.');
  process.exit(1);
}

// Create a new FormData object and append the ZIP file to it
const formData = new FormData();
formData.append('file', fs.createReadStream(APIGEE_BUNDLE_PATH), {
  filename: path.basename(APIGEE_BUNDLE_PATH)
});

// Define a function to upload the bundle to Apigee
async function uploadBundle() {
  // Create a new Axios instance with the APIGEE base URL and version, using a proxy
  const proxyAxios = axios.create({
    baseURL: `${APIGEE_BASE_URL}/${APIGEE_API_VERSION}`,
    httpsAgent: await createHttpsAgent(APIGEE_CERT_PATH, APIGEE_CERT_PASSWORD, APIGEE_PROXY_URL)
  });
  proxyAxios.defaults.headers.common['Authorization'] = `Bearer ${APIGEE_TOKEN}`;
  
  try {
    // Make a POST request to the Apigee API endpoint to upload the bundle, using the FormData object

const response = await apigeeAxios.post(/organizations/${APIGEE_ORG}/environments/${APIGEE_ENV}/apis/${APIGEE_PROXY_NAME}/revisions, formData, {
headers: {
...formData.getHeaders()
},
httpsAgent: new https.Agent({
cert: fs.readFileSync(APIGEE_CERT_PATH),
passphrase: APIGEE_CERT_PASSWORD
})
});

// Log a success message with the revision ID
logger.info(`Bundle uploaded successfully. Revision ID: ${response.data.revision}`);

// Return the revision ID
return response.data.revision;

} catch (error) {
// Log an error message and re-throw the error
logger.error(Error uploading bundle: ${error.message});
throw error;
}
}

// Define a function to upload the proxy to Apigee with retry logic
async function uploadProxyWithRetry() {
const maxRetries = 3;
let attempt = 1;

while (attempt <= maxRetries) {
try {
logger.info(Uploading bundle attempt ${attempt}...);
const revisionId = await uploadBundle();
return revisionId;
} catch (error) {
logger.error(Failed to upload bundle on attempt ${attempt}: ${error.message});
if (attempt === maxRetries) {
throw error;
}
logger.info(Retrying upload in 10 seconds...);
await new Promise(resolve => setTimeout(resolve, 10000));
attempt++;
}
}
}

// Define a function to upload the proxy bundle to Apigee using a proxy endpoint
async function uploadProxyBundle() {
try {
// Make a POST request to the Apigee proxy endpoint to upload the bundle
const response = await apigeeAxios.post(/uploadProxy/${APIGEE_ORG}/${APIGEE_ENV}/${APIGEE_PROXY_NAME}, formData, {
headers: {
...formData.getHeaders(),
Authorization: Bearer ${APIGEE_TOKEN}
},
httpsAgent: new https.Agent({
cert: fs.readFileSync(APIGEE_CERT_PATH),
passphrase: APIGEE_CERT_PASSWORD
})
});

// Log a success message with the revision ID
logger.info(`Bundle uploaded successfully using proxy. Revision ID: ${response.data.revision}`);

// Return the revision ID
return response.data.revision;
} catch (error) {
// Log an error message and re-throw the error
logger.error(Error uploading bundle using proxy: ${error.message});
throw error;
}
}

// Define a function to upload the proxy bundle to Apigee with retry logic
async function uploadProxyBundleWithRetry() {
const maxRetries = 3;
let attempt = 1;

while (attempt <= maxRetries) {
try {
logger.info(Uploading bundle using proxy attempt ${attempt}...);
const revisionId = await uploadProxyBundle();
return revisionId;
} catch (error) {
logger.error(Failed to upload bundle using proxy on attempt ${attempt}: ${error.message});
if (attempt === maxRetries) {
throw error;
}
logger.info(Retrying upload in 10 seconds...);
await new Promise(resolve => setTimeout(resolve, 10000));
attempt++;
}
}
}


async function createProxyRevision(currentRevision, newProperties) {
  try {
    // Create a new revision based on the current revision
    const newRevision = Object.assign({}, currentRevision);

    // Add the new properties to the new revision
    Object.keys(newProperties).forEach((key) => {
      newRevision[key] = newProperties[key];
    });

    // Return the new revision
    return newRevision;
  } catch (error) {
    console.error('An error occurred while creating a new revision:', error);
    return null;
  }
}




----
name: my-service
description: My GraphQL service
type: GRAPHQL

graphqlServiceConfig:
  url: https://my-graphql-endpoint.com
  authenticationType: APIKEY
  apiKey: ${env.HARNESS_GRAPHQL_API_KEY}

deploymentType: KUBERNETES
kubernetes:
  manifestType: HELM
  chartName: my-chart
  chartVersion: 1.0.0
  overrideValuesYaml: |
    image:
      repository: my-graphql-image
      tag: latest
    service:
      port: 8080
    replicaCount: 3


mutation {
  createWorkflow(
    workflow: {
      name: "My Workflow"
      applicationId: "abc123"
      workflowType: CLOUD_PROVIDER
      file: "{ contents of your workflow file }"
    }
    environmentId: "def456"
    serviceOverrideVariables: {
      overrideValues: true
      valuesOverride: "{ contents of your service overrides file }"
    }
    workflowOverrideVariables: {
      overrideValues: true
      valuesOverride: "{ contents of your workflow overrides file }"
    }
  ) {
    workflow {
      id
      name
      workflowType
    }
  }
}


mutation createPipeline {
  createPipeline(
    input: {
      name: "My Pipeline"
      applicationId: "my-application-id"
      pipelineType: CD
      workflowIds: ["my-workflow-id"]
      pipelineStages: [
        {
          stageName: "Deploy to Test",
          stageType: DEPLOY,
          serviceActionId: "my-service-action-id",
          targetSpec: {
            cloudProviderName: "kubernetes",
            namespace: "test"
          }
        },
        {
          stageName: "Deploy to Prod",
          stageType: DEPLOY,
          serviceActionId: "my-service-action-id",
          targetSpec: {
            cloudProviderName: "kubernetes",
            namespace: "prod"
          }
        }
      ]
    }
  ) {
    pipeline {
      id
      name
    }
  }
}




#!/bin/bash

# Fetch the new Jenkins Docker image and agent from OpenShift
oc login <OpenShift_URL> -u <username> -p $OPENSHIFT_PASSWORD
oc project <project_name>
oc import-image jenkins:latest --from=<openshift_repo>/jenkins:latest --confirm
oc import-image jenkins-agent-python:latest --from=<openshift_repo>/jenkins-agent-python:latest --confirm
oc import-image jenkins-agent-java:latest --from=<openshift_repo>/jenkins-agent-java:latest --confirm

# Start a Jenkins pod using the new image
oc new-app jenkins:latest

# Validate that the Jenkins pod is running
if oc get pod | grep -q "jenkins.*Running"; then
    echo "Jenkins is running"
else
    echo "Error: Jenkins is not running"
    exit 1
fi

# Run a Python build using the new agent
oc new-build --name=python-build --image-stream=python:latest --binary
oc start-build python-build --from-dir=. --follow

# Run a Java build using the new agent
oc new-build --name=java-build --image-stream=java:latest --binary
oc start-build java-build --from-dir=. --follow

export OPENSHIFT_PASSWORD=your_password


import os

def compare_files(dir1, dir2):
    differences = {}
    for filename in os.listdir(dir1):
        file1_path = os.path.join(dir1, filename)
        file2_path = os.path.join(dir2, filename)
        if os.path.isfile(file1_path) and os.path.isfile(file2_path):
            with open(file1_path) as file1, open(file2_path) as file2:
                lines1 = file1.readlines()
                lines2 = file2.readlines()
                if lines1 != lines2:
                    differences[filename] = [line for line in lines1 if line not in lines2]
    return differences

dir1 = "/path/to/dir1"
dir2 = "/path/to/dir2"
differences = compare_files(dir1, dir2)
print(differences)
-----


harness-charts/
├── environments/
│   ├── values.yaml
│   ├── prod.yaml
│   ├── stage.yaml
│   └── test.yaml
├── services/
│   ├── values.yaml
│   ├── db/
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   └── templates/
│   │       ├── deployment.yaml
│   │       ├── service.yaml
│   │       └── ...
│   ├── snowflake/
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   └── templates/
│   │       ├── deployment.yaml
│   │       ├── service.yaml
│   │       └── ...
│   ├── batch/
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   └── templates/
│   │       ├── job.yaml
│   │       ├── ...
│   └── ansible/
│       ├── Chart.yaml
│       ├── values.yaml
│       └── templates/
│           ├── deployment.yaml
│           ├── ...
├── workflows/
│   ├── values.yaml
│   ├── my_workflow/
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   └── templates/
│   │       ├── steps.yaml
│   │       ├── ...
│   ├── another_workflow/
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   └── templates/
│   │       ├── steps.yaml
│   │       ├── ...
│   └── ...
├── pipelines/
│   ├── values.yaml
│   ├── my_pipeline/
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   └── templates/
│   │       ├── stages.yaml
│   │       ├── ...
│   ├── another_pipeline/
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   └── templates/
│   │       ├── stages.yaml
│   │       ├── ...
│   └── ...
└── ...

harness-charts/
├── charts/
│   ├── snowflake/
│   │   ├── Chart.yaml
│   │   ├── templates/
│   │   │   ├── service.yaml
│   │   │   ├── workflow.yaml
│   │   │   ├── pipeline.yaml
│   │   ├── values.yaml
│   ├── database/
│   │   ├── Chart.yaml
│   │   ├── templates/
│   │   │   ├── service.yaml
│   │   │   ├── workflow.yaml
│   │   │   ├── pipeline.yaml
│   │   ├── values.yaml
├── environments/
│   ├── values.yaml
│   ├── dev.yaml
│   ├── test.yaml
│   ├── prod.yaml
├── values.yaml


The charts folder contains subfolders for each deployment type: snowflake and database. Each of these subfolders has a Chart.yaml file to define the chart metadata, a values.yaml file to define the default values for that deployment type, and a templates folder that contains the Kubernetes manifests for the services, workflows, and pipelines.

The environments folder contains YAML files for each environment, including a values.yaml file that defines the default values for all environments. The adopting user can override these default values by creating a separate YAML file for each environment (e.g., dev.yaml, test.yaml, prod.yaml) and specifying the overrides there.

Here's an example of what the snowflake/values.yaml file might look like for a contributor managing the Snowflake deployment type:

And here's an example of what the database/values.yaml file might look like for a contributor managing the database deployment type:
database:
  driver: "oracle.jdbc.driver.OracleDriver"
  url: "jdbc:oracle:thin:@//mydatabase.host.com:1521/ORCLCDB"
  username: "myuser"
  password: "mypassword"






# Override values for the DB service
helm upgrade --install db-service harness-charts/services/db -f my-db-values.yaml

# Override values for the "my_workflow" workflow
helm upgrade --install my-workflow harness-charts/workflows/my_workflow -f my-workflow-values.yaml

# Override values for the "my_pipeline" pipeline
helm upgrade --install my-pipeline harness-charts/pipelines/my_pipeline -f my-pipeline-values.yaml



charts/oracle/Chart.yaml

apiVersion: v2
name: oracle
description: Helm chart for deploying Oracle
version: 0.1.0

charts/oracle/values.yaml

database:
  driver: "oracle.jdbc.driver.OracleDriver"
  url: "jdbc:oracle:thin:@//mydatabase.host.com:1521/ORCLCDB"
  username: "myuser"
  password: "mypassword"

charts/oracle/templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: oracle
spec:
  ports:
    - name: oracle
      port: 1521
  selector:
    app: oracle

charts/oracle/templates/workflow.yaml

apiVersion: harness.io/v1
kind: Workflow
metadata:
  name: oracle-workflow
  annotations:
    description: Workflow for deploying Oracle
spec:
  applicationId: {{ .Values.applicationId }}
  pipelineStages:
  - type: DEPLOY
    name: deploy-oracle
    deployStage:
      accountName: {{ .Values.accountName }}
      strategy: ROLLING
      overrideValuesYaml: |
        image:
          repository: my-artifactory-instance.com/my-organization/oracle-chart
          tag: {{ .Values.chartVersion }}
      clusters:
      - clusterName: {{ .Values.clusterName }}
  triggers:
  - type: MANUAL



charts/oracle/templates/pipeline.yaml

apiVersion: harness.io/v1
kind: Pipeline
metadata:
  name: oracle-pipeline
  annotations:
    description: Pipeline for deploying Oracle using Kubernetes and Helm
spec:
  applicationId: {{ .Values.applicationId }}
  pipelineStages:
  - type: DEPLOY
    name: deploy-oracle
    deployStage:
      accountName: {{ .Values.accountName }}
      strategy: ROLLING
      overrideValuesYaml: |
        image:
          repository: my-artifactory-instance.com/my-organization/oracle-chart
          tag: {{ .Values.chartVersion }}
      clusters:
      - clusterName: {{ .Values.clusterName }}
  - type: DEPLOY
    name: deploy-oracle-service
    deployStage:
      accountName: {{ .Values.accountName }}
      strategy: ROLLING
      overrides: |
        {
          "service": {
            "values": {
              "image": {
                "repository": "my-artifactory-instance.com/my-organization/oracle-service-chart",
                "tag": "{{ .Values.serviceChartVersion }}"
              }
            }
          }
        }
      clusters:
      - clusterName: {{ .Values.clusterName }}
  triggers:
  - type: MANUAL



---



const axios = require('axios');
const fs = require('fs');
const { promisify } = require('util');
const exec = promisify(require('child_process').exec);

async function main() {
  try {
    // Fetch JSON data containing the latest chart version number with name
    const { data } = await axios.get('https://my-api.com/latest-chart-version');
    const chartName = data.name;
    const chartVersion = data.version;

    // Checkout Bitbucket code matching the chart name
    const { stdout: repoPath } = await exec(`git clone git@bitbucket.org:my-org/${chartName}.git`);
    await exec(`cd ${repoPath} && git checkout ${chartVersion}`);

    // Upload data to Harness.io using the Drift API
    const serviceYaml = fs.readFileSync(`${repoPath}/helm/chart/oracle/templates/service.yaml`, 'utf8');
    const workflowYaml = fs.readFileSync(`${repoPath}/helm/chart/oracle/templates/workflow.yaml`, 'utf8');
    const pipelineYaml = fs.readFileSync(`${repoPath}/helm/chart/oracle/templates/pipeline.yaml`, 'utf8');
    const driftApiUrl = 'https://driftapi.harness.io/v1alpha1';
    const harnessAccountId = 'my-account-id';
    const harnessApiKey = 'my-api-key';
    const headers = {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${harnessApiKey}`,
    };
    const serviceResponse = await axios.post(`${driftApiUrl}/accounts/${harnessAccountId}/services`, {
      name: chartName,
      configAsYaml: serviceYaml,
    }, { headers });
    console.log('Service uploaded:', serviceResponse.data);

    const workflowResponse = await axios.post(`${driftApiUrl}/accounts/${harnessAccountId}/workflows`, {
      name: chartName,
      configAsYaml: workflowYaml,
    }, { headers });
    console.log('Workflow uploaded:', workflowResponse.data);

    const pipelineResponse = await axios.post(`${driftApiUrl}/accounts/${harnessAccountId}/pipelines`, {
      name: chartName,
      configAsYaml: pipelineYaml,
    }, { headers });
    console.log('Pipeline uploaded:', pipelineResponse.data);

  } catch (error) {
    console.error(error);
  }
}

main();


-----
# General configuration
namespace: oracle
image: my-registry/oracle:latest
replicas: 2

# Database configuration
db:
  type: oracle
  name: mydb
  host: db.example.com
  port: 1521
  user: myuser
  password: mypassword

# Service configuration
service:
  type: LoadBalancer
  port: 8080
  healthCheckPath: /health

# Environment specific configuration
environments:
  - name: dev
    db:
      name: mydb-dev
      user: myuser-dev
      password: mypassword-dev
  - name: qa
    db:
      name: mydb-qa
      user: myuser-qa
      password: mypassword-qa
  - name: prod
    db:
      name: mydb-prod
      user: myuser-prod
      password: mypassword-prod
      replicas: 3
      
      
----

# Snowflake service values
snowflake:
  image:
    repository: myorg/snowflake
    tag: v1.0.0
  port: 8080
  replicas: 2
  # Additional service-specific values go here

# Workflow values
workflow:
  name: my-workflow
  workflowType: CONTINUOUS_DELIVERY
  # Additional workflow-specific values go here

# Pipeline values
pipeline:
  name: my-pipeline
  trigger: MANUAL
  # Additional pipeline-specific values go here

# Common values for all components
global:
  environment: dev
  # Additional global values go here

====

# Default values for Harness charts.
# This is a YAML-formatted file.

global:
  namespace: "default"
  app_name: "my-app"
  app_env: "dev"

environments:
  - name: "dev"
    type: "KUBERNETES"
    namespace: "dev-namespace"
    label_selector: "env=dev"
    # Optional: Define the Kubernetes service account to use for the deployment.
    # service_account_name: ""

  - name: "test"
    type: "KUBERNETES"
    namespace: "test-namespace"
    label_selector: "env=test"
    # Optional: Define the Kubernetes service account to use for the deployment.
    # service_account_name: ""

  - name: "prod"
    type: "KUBERNETES"
    namespace: "prod-namespace"
    label_selector: "env=prod"
    # Optional: Define the Kubernetes service account to use for the deployment.
    # service_account_name: ""

services:
  - name: "my-service"
    chart: "snowflake"
    chart_version: "1.0.0"
    # Optional: Define any overrides to the chart values.
    values_file: "service-overrides.yaml"

workflows:
  - name: "my-workflow"
    service_name: "my-service"
    chart: "oracle"
    chart_version: "1.0.0"
    # Optional: Define any overrides to the chart values.
    values_file: "workflow-overrides.yaml"

pipelines:
  - name: "my-pipeline"
    workflow_name: "my-workflow"
    environments:
      - name: "dev"
      - name: "test"
      - name: "prod"
    # Optional: Define an approval step before promoting to the next environment.
    approval_step:
      type: "MANUAL"
      name: "my-approval-step"
      description: "Please approve the deployment to the next environment."
      approver_email: "user@example.com"


